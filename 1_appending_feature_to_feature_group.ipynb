{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ec3f3a-7558-43ce-8ec5-303ceccf75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55620b-9c1d-4ed9-8a74-3c5e5ea8b84d",
   "metadata": {},
   "source": [
    "## Creating a feature group from the feature group with Json Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987b0d6-e963-486f-8c10-3beeca18ca0d",
   "metadata": {},
   "source": [
    "#### Login to Hopsworks and fetch feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977113bf-dca4-481c-aa25-5b389d2382b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "2025-03-12 20:18:51,407 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://hopsworks.ai.local/p/119\n"
     ]
    }
   ],
   "source": [
    "# Login to Hopsworks.\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Fetch the feature store.\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Fetch the feature group\n",
    "fg_json = fs.get_feature_group(name=\"fg_raw_event_data\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13150da0-5545-4572-a8e0-2c8c5b72f344",
   "metadata": {},
   "source": [
    "#### Read data from the feature group and perform required feature enginerring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df17a41f-353e-42e3-a3f7-7bd2398c9dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.18s) \n"
     ]
    }
   ],
   "source": [
    "# Read from feature group\n",
    "df = fg_json.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe68cbc4-4814-4306-ab27-11bddf29c3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_count</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>scroll_depth</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>ad_interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-12T17:10:57.851375</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>3.06</td>\n",
       "      <td>42.19</td>\n",
       "      <td>14.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-12T17:10:57.851693</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>19.84</td>\n",
       "      <td>39.56</td>\n",
       "      <td>457.27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   event_time  event_id  user_id  click_count  time_spent  \\\n",
       "0  2025-03-12T17:10:57.851375         1        4           17        3.06   \n",
       "1  2025-03-12T17:10:57.851693        17       11            6       19.84   \n",
       "\n",
       "   scroll_depth  purchase_amount  ad_interaction  \n",
       "0         42.19            14.90             NaN  \n",
       "1         39.56           457.27             NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the json data convert it into a dataframe with required columns\n",
    "unnested_dataframe = pd.json_normalize(df[\"data\"].apply(lambda x : json.loads(x)))\n",
    "unnested_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6ac2079-b216-48ac-bb3a-4ce614714f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform required feature enginering\n",
    "unnested_dataframe = unnested_dataframe[[\"event_time\", \"event_id\", \"user_id\", \"click_count\", \"time_spent\", \"scroll_depth\", \"purchase_amount\"]]\n",
    "# Convert string datetime to datetime object\n",
    "unnested_dataframe[\"event_time\"] = pd.to_datetime(unnested_dataframe[\"event_time\"])\n",
    "\n",
    "user_event_df = unnested_dataframe[[\"event_time\", \"event_id\", \"user_id\", \"purchase_amount\"]]\n",
    "\n",
    "events_df = unnested_dataframe[[\"event_time\", \"event_id\", \"click_count\", \"time_spent\", \"scroll_depth\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99ce08-338f-43c0-b1e6-0892f689c5fe",
   "metadata": {},
   "source": [
    "#### Create feature groups\n",
    "\n",
    "There are two feature groups being created.\n",
    "1. **An user-events feature group** : This feature group store all events for an user. The feature group has the primary key as `user_id` hence the online feature store will only contain the latest events for the user and the offline feature group will contain all event triggered by the user.\n",
    "2. **An events feature group**: This feature group will conatin all information regarding the event. This feature group will have the primary key as `event_id`. Hence can be joined with the user-events feature group to create the entire dataframe.\n",
    "\n",
    "Splitting the data like this also allows creating a seperate *users feature group* which could contain user specific details which can again be joined to create a feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82639f3b-5377-48b6-b675-70e38b510f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 20:59:12,809 WARNING: DeprecationWarning: Providing event_time as a single-element list is deprecated and will be dropped in future versions. Provide the feature_name string instead.\n",
      "\n",
      "Feature Group created successfully, explore it at \n",
      "https://hopsworks.ai.local/p/119/fs/67/fg/26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 40/40 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: fg_user_events_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks.ai.local/p/119/jobs/named/fg_user_events_1_offline_fg_materialization/executions\n",
      "Feature Group created successfully, explore it at \n",
      "https://hopsworks.ai.local/p/119/fs/67/fg/27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 40/40 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: fg_events_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks.ai.local/p/119/jobs/named/fg_events_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('fg_events_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating user-events feature group\n",
    "fg_user_events = fs.get_or_create_feature_group(name = \"fg_user_events\",\n",
    "                                                version = 1, \n",
    "                                                primary_key = [\"user_id\"],\n",
    "                                                event_time = [\"event_time\"],\n",
    "                                                online_enabled=True)\n",
    "# Creating events feature group\n",
    "fg_events = fs.get_or_create_feature_group(name = \"fg_events\",\n",
    "                                                version = 1, \n",
    "                                                primary_key = [\"event_id\"],\n",
    "                                                event_time = [\"event_time\"],\n",
    "                                                online_enabled=True)\n",
    "\n",
    "\n",
    "# Inserting data into the feature groups\n",
    "fg_user_events.insert(user_event_df)\n",
    "fg_events.insert(events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a4a54-a6cc-4704-87fb-f8b46981ae27",
   "metadata": {},
   "source": [
    "## Creating a feature view and generating train-test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0cf43-653d-40ba-87ba-ed1ed7194cd7",
   "metadata": {},
   "source": [
    "#### Define a query to join feature groups\n",
    "\n",
    "The joins performed by Hopsworks are always point in time correct base on event time. Hence you can easily join the the user-events and the users feature groups to create a new feature view that has point int time correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54bbeea-b0ea-46d3-9a22-f3492b5984e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 21:01:10,775 INFO: Using ['click_count', 'time_spent', 'scroll_depth'] as features for the query.To include primary key and event time use `select_all`.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.39s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>event_data_click_count</th>\n",
       "      <th>event_data_time_spent</th>\n",
       "      <th>event_data_scroll_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.77</td>\n",
       "      <td>0</td>\n",
       "      <td>22.38</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177.64</td>\n",
       "      <td>20</td>\n",
       "      <td>25.11</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>438.18</td>\n",
       "      <td>2</td>\n",
       "      <td>6.72</td>\n",
       "      <td>94.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304.49</td>\n",
       "      <td>20</td>\n",
       "      <td>9.30</td>\n",
       "      <td>63.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157.73</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>3.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_amount  event_data_click_count  event_data_time_spent  \\\n",
       "0            69.77                       0                  22.38   \n",
       "1           177.64                      20                  25.11   \n",
       "2           438.18                       2                   6.72   \n",
       "3           304.49                      20                   9.30   \n",
       "4           157.73                       1                   7.26   \n",
       "\n",
       "   event_data_scroll_depth  \n",
       "0                    24.49  \n",
       "1                    16.27  \n",
       "2                    94.29  \n",
       "3                    63.89  \n",
       "4                     3.21  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = fg_user_events.select(\"purchase_amount\").join(fg_events.select_features(), prefix=\"event_data_\", on=\"event_id\")\n",
    "query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dcef4-cdd4-4688-b00f-94d2f9016b41",
   "metadata": {},
   "source": [
    "#### Creating feature view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "328369b5-2127-45e6-8681-e7fd89104694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.29s) \n",
      "2025-03-12 21:04:46,580 WARNING: VersionWarning: Incremented version to `1`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import any require model-dependent transformation functions\n",
    "from hopsworks.hsfs.builtin_transformations import min_max_scaler\n",
    "\n",
    "# Create feature view\n",
    "fv = fs.get_or_create_feature_view(name=\"fv_events\", \n",
    "                                   version = 1, \n",
    "                                   query = query, \n",
    "                                   transformation_functions=[\n",
    "                                       min_max_scaler(\"event_data_click_count\"), \n",
    "                                       min_max_scaler(\"event_data_time_spent\"), \n",
    "                                       min_max_scaler(\"event_data_scroll_depth\")\n",
    "                                   ],\n",
    "                                   labels = [\"purchase_amount\"])\n",
    "\n",
    "\n",
    "# Create training data\n",
    "X_train, X_test, y_train, y_test = fv.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2397a6-f4ac-4e7a-a199-c6eb729608cc",
   "metadata": {},
   "source": [
    "## Testing a new feature\n",
    "\n",
    "You can easily test a new fetaure without disturbing existing feature groups by creating a seperate feature group with only the new tests feature, this feature group can be joined with existing feature groups to create a new feature view. Once testing is done, the new feature can then be appended to an existing feature group can be used to create and backfill an new feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2d6c2dd-d062-4d95-906b-157d8eafa4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.23s) \n"
     ]
    }
   ],
   "source": [
    "# Fetch the feature group with the raw json\n",
    "fg_json = fs.get_feature_group(name=\"fg_raw_event_data\", version=1)\n",
    "\n",
    "# Read the data from the raw feature group.\n",
    "df = fg_json.read()\n",
    "unnested_dataframe = pd.json_normalize(df[\"data\"].apply(lambda x : json.loads(x)))\n",
    "unnested_dataframe[\"event_time\"] = pd.to_datetime(unnested_dataframe[\"event_time\"])\n",
    "\n",
    "# Extract the new feature that is required to be tested.\n",
    "test_df = unnested_dataframe[[\"event_time\", \"event_id\", \"ad_interaction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd8c25fd-1fb3-4971-af84-b5720ecb96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 21:17:11,692 WARNING: DeprecationWarning: Providing event_time as a single-element list is deprecated and will be dropped in future versions. Provide the feature_name string instead.\n",
      "\n",
      "Feature Group created successfully, explore it at \n",
      "https://hopsworks.ai.local/p/119/fs/67/fg/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 40/40 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: fg_test_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks.ai.local/p/119/jobs/named/fg_test_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('fg_test_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the new test feature_group\n",
    "fg_test = fs.get_or_create_feature_group(name = \"fg_test\",\n",
    "                                                version = 1, \n",
    "                                                primary_key = [\"event_id\"],\n",
    "                                                event_time = [\"event_time\"],\n",
    "                                                online_enabled=True)\n",
    "\n",
    "fg_test.insert(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb4c496b-8b27-4ed1-8dcc-c2e62403c888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 21:21:47,220 INFO: Using ['click_count', 'time_spent', 'scroll_depth'] as features for the query.To include primary key and event time use `select_all`.\n",
      "2025-03-12 21:21:47,221 INFO: Using ['ad_interaction'] as features for the query.To include primary key and event time use `select_all`.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.42s) \n"
     ]
    }
   ],
   "source": [
    "# Create a new testing feature view that joins the feature groups : fg_user_events, fg_events and fg_test\n",
    "query = fg_user_events.select(\"purchase_amount\").join(fg_events.select_features(), prefix=\"event_data_\", on=\"event_id\").join(fg_test.select_features(), prefix=\"testing_\", on=\"event_id\")\n",
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dea14b08-a873-4111-92ff-9c09e7765938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://hopsworks.ai.local/p/119/fs/67/fv/fv_test/version/1\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.37s) \n",
      "2025-03-12 21:22:49,359 WARNING: VersionWarning: Incremented version to `1`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create testing feature view\n",
    "fv_test = fs.get_or_create_feature_view(name=\"fv_test\", \n",
    "                                   version = 1, \n",
    "                                   query = query, \n",
    "                                   transformation_functions=[\n",
    "                                       min_max_scaler(\"event_data_click_count\"), \n",
    "                                       min_max_scaler(\"event_data_time_spent\"), \n",
    "                                       min_max_scaler(\"event_data_scroll_depth\"),\n",
    "                                       min_max_scaler(\"testing_ad_interaction\")\n",
    "                                   ],\n",
    "                                   labels = [\"purchase_amount\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = fv_test.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f75095-487e-4a47-a792-6e038080ad72",
   "metadata": {},
   "source": [
    "### Appending new features to the feature view\n",
    "Once a feature has been testing and wants to be included in the feature group it can be done appending the feature to the feature group or by creating a new version of the feature group that contains the new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4d68301-86a1-4d00-8485-6f6fc18271a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopsworks.hsfs.feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc39fe28-64cb-4ff5-a992-f2eb88485130",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [Feature(name=\"ad_interaction\",type=\"double\",online_type=\"double\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85ad4d7b-3064-4b7a-b2ab-3085cdb6caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 21:32:12,791 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-03-12 21:32:15,838 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-03-12 21:33:16,697 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-03-12 21:33:16,732 INFO: Waiting for log aggregation to finish.\n",
      "2025-03-12 21:33:24,964 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_group.FeatureGroup at 0x7f5517d038e0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appending a new feature to the feature group\n",
    "fg_events.append_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "028bc254-b002-4861-83ec-41aeed9989ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.22s) \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Features are not compatible with Feature Group schema: \n - ad_interaction (expected type: 'int', derived from input: 'double') has the wrong type.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m events_df \u001b[38;5;241m=\u001b[39m unnested_dataframe[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclick_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscroll_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mad_interaction\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Insert the data into the feature group\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mfg_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/hopsworks_environment/lib/python3.10/site-packages/hsfs/feature_group.py:3009\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[0m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[0;32m-> 3009\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[0;32m~/envs/hopsworks_environment/lib/python3.10/site-packages/hsfs/core/feature_group_engine.py:188\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_feature_group_metadata(\n\u001b[1;32m    184\u001b[0m         feature_group, dataframe_features, write_options\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_schema_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# ge validation on python and non stream feature groups on spark\u001b[39;00m\n\u001b[1;32m    193\u001b[0m ge_report \u001b[38;5;241m=\u001b[39m feature_group\u001b[38;5;241m.\u001b[39m_great_expectation_engine\u001b[38;5;241m.\u001b[39mvalidate(\n\u001b[1;32m    194\u001b[0m     feature_group\u001b[38;5;241m=\u001b[39mfeature_group,\n\u001b[1;32m    195\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mfeature_dataframe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     ge_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m~/envs/hopsworks_environment/lib/python3.10/site-packages/hsfs/core/feature_group_base_engine.py:186\u001b[0m, in \u001b[0;36mFeatureGroupBaseEngine._verify_schema_compatibility\u001b[0;34m(self, feature_group_features, dataframe_features)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# raise exception if any errors were found.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(err) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures are not compatible with Feature Group schema: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m err])\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote that feature (or column) names are case insensitive and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces are automatically replaced with underscores.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Features are not compatible with Feature Group schema: \n - ad_interaction (expected type: 'int', derived from input: 'double') has the wrong type.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores."
     ]
    }
   ],
   "source": [
    "# Now backfill the required data into the feature group\n",
    "# Read the data from the raw feature group.\n",
    "df = fg_json.read()\n",
    "unnested_dataframe = pd.json_normalize(df[\"data\"].apply(lambda x : json.loads(x)))\n",
    "unnested_dataframe[\"event_time\"] = pd.to_datetime(unnested_dataframe[\"event_time\"])\n",
    "\n",
    "events_df = unnested_dataframe[[\"event_time\", \"event_id\", \"click_count\", \"time_spent\", \"scroll_depth\", \"ad_interaction\"]]\n",
    "\n",
    "# Insert the data into the feature group\n",
    "fg_events.insert(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56e693a0-007d-4662-a0d9-152305306a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 21:37:25,836 WARNING: DeprecationWarning: Providing event_time as a single-element list is deprecated and will be dropped in future versions. Provide the feature_name string instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Creating a new version of the feature group\n",
    "\n",
    "# Creating events feature group\n",
    "fg_events_v2 = fs.get_or_create_feature_group(name = \"fg_events\",\n",
    "                                                version = 2, \n",
    "                                                primary_key = [\"event_id\"],\n",
    "                                                event_time = [\"event_time\"],\n",
    "                                                online_enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fec4b483-af1a-4c71-a737-41716f66e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.21s) \n",
      "Feature Group created successfully, explore it at \n",
      "https://hopsworks.ai.local/p/119/fs/67/fg/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 40/40 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: fg_events_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks.ai.local/p/119/jobs/named/fg_events_2_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('fg_events_2_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the required data into the new feature group version\n",
    "# Read the data from the raw feature group.\n",
    "df = fg_json.read()\n",
    "unnested_dataframe = pd.json_normalize(df[\"data\"].apply(lambda x : json.loads(x)))\n",
    "unnested_dataframe[\"event_time\"] = pd.to_datetime(unnested_dataframe[\"event_time\"])\n",
    "\n",
    "events_df = unnested_dataframe[[\"event_time\", \"event_id\", \"click_count\", \"time_spent\", \"scroll_depth\", \"ad_interaction\"]]\n",
    "\n",
    "# Insert the data into the feature group\n",
    "fg_events_v2.insert(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933843e-1409-4659-8e0b-8efa42e09f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
